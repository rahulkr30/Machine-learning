{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:Pink'> Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is a fundamental metric used to assess the performance of a classification model. It measures the proportion of correctly classified instances out of the total number of instances in a dataset. In other words, it tells us how often the model's predictions are correct.\n",
    " > example:- \n",
    " Imagine you have a dataset with 100 instances, and you apply a classification model to predict the class labels for these instances. The model correctly predicts 85 instances, while it makes mistakes on the remaining 15 instances. The accuracy of the model can be calculated by dividing the number of correctly predicted instances (85) by the total number of instances (100), which gives us an accuracy of 85%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Confusion matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| Actual Positive| TP                 | FN                 |\n",
    "| Actual Negative| FP                 | TN                 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, accuracy is defined as:\n",
    "\n",
    "\n",
    "$ Accuracy = \\frac{Correct\\_prediction}{Total\\_prediction} $\n",
    "```\n",
    "\n",
    "This formula calculates the ratio of correct predictions to the total number of predictions made by the model. A higher accuracy means that the model is more reliable and accurate.\n",
    "```\n",
    "$Accuracy = (TP + TN) / (TP + TN + FP + FN)$\n",
    "\n",
    "where:\n",
    "\n",
    "- TP refers to the number of true positives, i.e., the instances that are correctly predicted as positive.\n",
    "- TN refers to the number of true negatives, i.e., the instances that are correctly predicted as negative.\n",
    "- FP refers to the number of false positives, i.e., the instances that are incorrectly predicted as positive when they are actually negative.\n",
    "- FN refers to the number of false negatives, i.e., the instances that are incorrectly predicted as negative when they are actually positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score , precision_score, recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_classification(n_samples=1000,n_features=20,n_informative=10,n_classes=2,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = [i for i in range (1,20,2)]\n",
    "score=[]\n",
    "prediction=[]\n",
    "for k in (k_value):\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    score.append(knn.score(X_test,y_test))\n",
    "    prediction.append(knn.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8766666666666667,\n",
       " 0.92,\n",
       " 0.9066666666666666,\n",
       " 0.9033333333333333,\n",
       " 0.91,\n",
       " 0.92,\n",
       " 0.91,\n",
       " 0.9166666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.92]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8766666666666667\n",
      "0.92\n",
      "0.9066666666666666\n",
      "0.9033333333333333\n",
      "0.91\n",
      "0.92\n",
      "0.91\n",
      "0.9166666666666666\n",
      "0.9133333333333333\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "for i in prediction:\n",
    "    accuracy=accuracy_score(i,y_test)\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
