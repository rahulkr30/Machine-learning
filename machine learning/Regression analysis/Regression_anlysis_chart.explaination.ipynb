{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>F-Statistics</b>\n",
    "In linear regression analysis, the F-statistic is a statistical test used to determine if the overall linear regression model is significant or not. Specifically, it tests the <span style=\"color:blue\">Null hypothesis:</span> <span style='color:red'>All of the regression coefficients in the model are equal to zero.</span> indicating that there is no relationship between the independent variables and the dependent variable.\n",
    "\n",
    "When the F-statistic is calculated, it takes into account the variation in the dependent variable that is explained by the model (i.e., the \"sum of squares due to regression\") as well as the variation that is not explained by the model (i.e., the \"sum of squares due to error\"). If the F-statistic is large enough, it suggests that the variation explained by the model is significantly greater than the variation not explained by the model, indicating that there is a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "Therefore, the F-statistic does not directly give information about the linearity of the relationship between the independent variables and the dependent variable. Instead, it tests whether the overall linear regression model is significant or not. However, if the F-statistic is significant, it suggests that there is evidence of a linear relationship between the independent variables and the dependent variable, and further analysis of the regression coefficients and other diagnostic measures can be used to examine the nature and strength of this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 521.7033489265004\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.842\n",
      "Model:                            OLS   Adj. R-squared:                  0.840\n",
      "Method:                 Least Squares   F-statistic:                     521.7\n",
      "Date:                Tue, 16 May 2023   Prob (F-statistic):           4.95e-41\n",
      "Time:                        10:20:40   Log-Likelihood:                -138.83\n",
      "No. Observations:                 100   AIC:                             281.7\n",
      "Df Residuals:                      98   BIC:                             286.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0191      0.098     -0.195      0.846      -0.214       0.175\n",
      "x1             1.9834      0.087     22.841      0.000       1.811       2.156\n",
      "==============================================================================\n",
      "Omnibus:                        5.027   Durbin-Watson:                   1.860\n",
      "Prob(Omnibus):                  0.081   Jarque-Bera (JB):                5.131\n",
      "Skew:                          -0.308   Prob(JB):                       0.0769\n",
      "Kurtosis:                       3.924   Cond. No.                         1.13\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(123)\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = 2*x + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Fit a linear regression model\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Calculate the F-statistic and p-value\n",
    "f_statistic = model.fvalue\n",
    "p_value = model.f_pvalue\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521.7033489265004\n"
     ]
    }
   ],
   "source": [
    "lr=LinearRegression().fit(x.reshape(-1, 1),y)\n",
    "Y_hat=lr.predict(x.reshape(-1, 1))\n",
    "k=1\n",
    "n=len(x)\n",
    "Ess=np.sum((Y_hat - np.mean(y))**2)\n",
    "Rrs=np.sum((y - Y_hat)**2)\n",
    "\n",
    "f_statistics=(Ess/k)/(Rrs/(n-k-1))\n",
    "print(f_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.952865753448413e-41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.f.sf(f_statistics,k,n-k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(1-scipy.stats.f.cdf(f_statistics,k,n-k-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 score "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The R-squared (R2) Score\n",
    "\n",
    "The R-squared (R2) score is a statistical measure used to evaluate the goodness of fit of a linear regression model. It provides an indication of how well the observed data fits the regression model and how well the independent variables explain the variation in the dependent variable.\n",
    "\n",
    "The R2 score is a number between 0 and 1, with higher values indicating a better fit of the model.\n",
    "\n",
    "### Mathematical Derivation\n",
    "Mathematically, the R2 score is defined as the proportion of the total variance in the dependent variable that is explained by the independent variables in the regression model. It is calculated using the following formula:\n",
    "\n",
    "\n",
    "​R2 = {$SS_{exp}$}/{$SS_{tot}$} \n",
    "\n",
    "where $SS_{exp}$ is the sum of squares explained, $SS_{res}$ is the sum of squared residuals (i.e., the sum of the squared differences between the observed values and the predicted values), and $SS_{tot}$ is the total sum of squares (i.e., the sum of the squared differences between the observed values and the mean value of the dependent variable).\n",
    "\n",
    "The total sum of squares can be expressed as the sum of the explained sum of squares and the residual sum of squares:\n",
    "\n",
    "\n",
    "​$SS_{tot}$ = $SS_{exp}$ + $SS_{res}$\n",
    " \n",
    "\n",
    "Therefore, the R2 score can also be calculated using the following formula:\n",
    "\n",
    "R2 = 1- ({$SS_{res}$}/{$SS_{tot}$})\n",
    "\n",
    "### Interpretation of R2 Score\n",
    "The R2 score ranges from 0 to 1, where an R2 of 0 means that none of the variation in the dependent variable is explained by the independent variables in the model, and an R2 of 1 means that all of the variation in the dependent variable is explained by the independent variables in the model.\n",
    "\n",
    "Generally, an R2 score above 0.7 is considered a good fit for a regression model, but the threshold may vary depending on the context and the field of study. It is important to note that the R2 score alone does not provide information about the statistical significance of the regression coefficients or the overall significance of the regression model. Therefore, it should be used in conjunction with other statistical tests and diagnostic measures to evaluate the fit of the regression model and the significance of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8418598186216624"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y,Y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where R2 score is fail \n",
    "\n",
    "- One situation is when the model is overfitted to the data. This happens when the model is too complex and fits too closely to the specific data points in the sample. In this case, the R2 score may be high, but the model may not generalize well to new data outside of the sample.\n",
    "\n",
    "- Another situation is when the relationship between the independent variables and the dependent variable is not linear. The R2 score only measures the goodness of fit of a linear regression model, and does not account for other types of relationships between the variables. This means that the R2 score may be low even if there is a strong relationship between the variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``understand by an example ``\n",
    "\n",
    "<span style='color:pink'> When there is an irrelevant column (also known as a \"feature\" or \"predictor variable\") in the linear regression model, it can artificially inflate the R2 score, making it appear that the model is a good fit for the data. This is because the irrelevant column does not contribute to the prediction of the dependent variable, but it does add an extra degree of freedom to the model, which can increase the R2 score.\n",
    "\n",
    "<span style='color:pink'>However, in reality, the irrelevant column is not useful for predicting the dependent variable, and including it in the model can actually reduce the performance of the model. This is because it can introduce noise and make it more difficult for the model to identify the relevant patterns in the data.\n",
    "\n",
    "<span style='color:pink'>So, even though the R2 score may not change when an irrelevant column is added to the model, it is important to carefully evaluate the relevance of each feature in the model to ensure that it is contributing to the prediction of the dependent variable. This can be done using techniques such as feature selection or regularization, which aim to identify and remove irrelevant or redundant features from the model.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a copy of the dataset with an irrelevant feature added\n",
    "X_with_irrelevant_feature = np.hstack((X, np.random.rand(X.shape[0], 1003)))\n",
    "\n",
    "# Train a linear regression model on the original dataset\n",
    "lr = LinearRegression().fit(X, y)\n",
    "y_pred_orig = lr.predict(X)\n",
    "r2_orig = r2_score(y, y_pred_orig)\n",
    "\n",
    "# Train a linear regression model on the dataset with the irrelevant feature added\n",
    "lr_with_irrelevant = LinearRegression().fit(X_with_irrelevant_feature, y)\n",
    "y_pred_irrelevant = lr_with_irrelevant.predict(X_with_irrelevant_feature)\n",
    "r2_with_irrelevant = r2_score(y, y_pred_irrelevant)\n",
    "\n",
    "# Perform feature selection using F-test to identify the relevant features\n",
    "f_values, p_values = f_regression(X, y)\n",
    "relevant_features = np.argwhere(p_values < 0.05).flatten()\n",
    "\n",
    "# Train a linear regression model using only the relevant features\n",
    "X_relevant = X.iloc[:, relevant_features]\n",
    "lr_relevant = LinearRegression().fit(X_relevant, y)\n",
    "y_pred_relevant = lr_relevant.predict(X_relevant)\n",
    "r2_relevant = r2_score(y, y_pred_relevant)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606232685199805\n",
      "0.6260654099664034\n",
      "0.6260654099664034\n"
     ]
    }
   ],
   "source": [
    "print(r2_orig)\n",
    "print(r2_with_irrelevant)\n",
    "print(r2_with_irrelevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted r2 square \n",
    "\n",
    "adjusted R2 can also be a good alternative to regular R2 when dealing with models that have irrelevant features. Adjusted R2 is a modified version of R2 that takes into account the number of predictor variables in the model. Unlike R2, adjusted R2 penalizes the inclusion of irrelevant features, which can help to mitigate the issue of R2 not capturing the true performance of the model in the presence of irrelevant feature\n",
    "\n",
    "$ Adj R2 = 1 - [(1 - R2) * (n - 1) / (n - p - 1)] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606079995629818\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "p = len(relevant_features)\n",
    "adj_r2_orig = 1 - ((1 - r2_orig) * (n - 1) / (n - p - 1))\n",
    "print(adj_r2_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6068047685090991\n"
     ]
    }
   ],
   "source": [
    "p_irr=X_with_irrelevant_feature.shape[1]\n",
    "adj_r2_irr = 1 - ((1 - r2_with_irrelevant) * (n - 1) / (n - p_irr - 1))\n",
    "print(adj_r2_irr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-Test (idividual indepentent variable  is  relation with dependent variable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test is a statistical test used in linear regression to determine if a particular predictor variable is significantly associated with the target variable. In other words, it helps us determine whether the coefficient of a predictor variable is statistically different from zero.\n",
    "\n",
    "<span style=\"color:pink\">The null hypothesis of the t-test is that the coefficient of the predictor variable is zero, meaning that there is no significant relationship between the predictor variable and the target variable. The alternative hypothesis is that the coefficient is non-zero, indicating that there is a significant relationship between the predictor variable and the target variable.\n",
    "\n",
    "To conduct a t-test in linear regression, we calculate the t-statistic by dividing the coefficient of the predictor variable by its standard error. The standard error measures the variability of the estimate of the coefficient, and the t-statistic measures how many standard errors the estimate is away from zero.\n",
    "\n",
    "If the t-statistic is large and positive or large and negative, it indicates that the coefficient is significantly different from zero, and we reject the null hypothesis. If the t-statistic is small or close to zero, it indicates that the coefficient is not significantly different from zero, and we fail to reject the null hypothesis.\n",
    "\n",
    "The t-test is useful for determining which predictor variables are most important in explaining the variability of the target variable, and can help us identify which variables to include in our regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical explaination \n",
    "\n",
    "In linear regression, we model the relationship between a dependent variable Y and one or more independent variables X. For a simple linear regression model with one independent variable, the model is given by:\n",
    "\n",
    "Y = β0 + β1X + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, β0 and β1 are the intercept and slope coefficients, and ε is the error term.\n",
    "\n",
    "To determine if the slope coefficient β1 is significantly different from zero, we conduct a t-test. The t-test is based on the t-statistic, which is calculated as:\n",
    "\n",
    "t = (β1 - 0) / SE(β1)\n",
    "\n",
    "where β1 is the estimated slope coefficient, 0 is the null hypothesis value (which is zero in this case), and SE(β1) is the standard error of the slope coefficient.\n",
    "\n",
    "The standard error of the slope coefficient is calculated as:\n",
    "\n",
    "SE(β1) = sqrt[Σ(yi - ŷi)^2 / (n - 2)] / sqrt[Σ(xi - x̄)^2]\n",
    "\n",
    "where yi is the observed value of Y, ŷi is the predicted value of Y, xi is the observed value of X, x̄ is the mean value of X, and n is the sample size.\n",
    "\n",
    "The t-statistic follows a t-distribution with n - 2 degrees of freedom. We can use this distribution to calculate the p-value of the t-test, which represents the probability of observing a t-statistic as extreme or more extreme than the one we calculated, assuming the null hypothesis is true.\n",
    "\n",
    "#  Interpretation   \n",
    "\n",
    "If the p-value is less than our chosen significance level (e.g., 0.05), we reject the null hypothesis and conclude that the slope coefficient is significantly different from zero, indicating that there is a significant relationship between the independent variable X and the dependent variable Y. If the p-value is greater than our significance level, we fail to reject the null hypothesis and conclude that there is insufficient evidence to suggest a significant relationship between X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coef_=lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc:\n",
      "Coefficient = 0.4367\n",
      "Standard Error = 0.0042\n",
      "t-Statistic = 104.0538\n",
      "p-value = 0.0000\n",
      "HouseAge:\n",
      "Coefficient = 0.0094\n",
      "Standard Error = 0.0004\n",
      "t-Statistic = 21.1432\n",
      "p-value = 0.0000\n",
      "AveRooms:\n",
      "Coefficient = -0.1073\n",
      "Standard Error = 0.0059\n",
      "t-Statistic = -18.2354\n",
      "p-value = 0.0000\n",
      "AveBedrms:\n",
      "Coefficient = 0.6451\n",
      "Standard Error = 0.0281\n",
      "t-Statistic = 22.9276\n",
      "p-value = 0.0000\n",
      "Population:\n",
      "Coefficient = -0.0000\n",
      "Standard Error = 0.0000\n",
      "t-Statistic = -0.8373\n",
      "p-value = 0.4024\n",
      "AveOccup:\n",
      "Coefficient = -0.0038\n",
      "Standard Error = 0.0005\n",
      "t-Statistic = -7.7686\n",
      "p-value = 0.0000\n",
      "Latitude:\n",
      "Coefficient = -0.4213\n",
      "Standard Error = 0.0072\n",
      "t-Statistic = -58.5414\n",
      "p-value = 0.0000\n",
      "Longitude:\n",
      "Coefficient = -0.4345\n",
      "Standard Error = 0.0075\n",
      "t-Statistic = -57.6822\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train a linear regression model on the original dataset\n",
    "lr = LinearRegression().fit(X, y)\n",
    "y_pred_orig = lr.predict(X)\n",
    "n = len(y)\n",
    "p = X.shape[1]\n",
    "df = n - p - 1\n",
    "r2_orig = r2_score(y, y_pred_orig)\n",
    "\n",
    "# Compute the standard error and t-test for each coefficient\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_diff = X - X_mean\n",
    "sse = np.sum((y - y_pred_orig) ** 2) / df\n",
    "se = np.sqrt(np.diag(sse * np.linalg.inv(np.dot(X_diff.T, X_diff))))\n",
    "t_stat = lr.coef_ / se\n",
    "p_values = 2 * t.sf(np.abs(t_stat), df)\n",
    "\n",
    "# Print the results\n",
    "for i, col_name in enumerate(X.columns):\n",
    "    print(f'{col_name}:')\n",
    "    print(f'Coefficient = {lr.coef_[i]:.4f}')\n",
    "    print(f'Standard Error = {se[i]:.4f}')\n",
    "    print(f't-Statistic = {t_stat[i]:.4f}')\n",
    "    print(f'p-value = {p_values[i]:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            MedHouseVal   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.606\n",
      "Method:                 Least Squares   F-statistic:                     3970.\n",
      "Date:                Wed, 10 May 2023   Prob (F-statistic):               0.00\n",
      "Time:                        22:31:02   Log-Likelihood:                -22624.\n",
      "No. Observations:               20640   AIC:                         4.527e+04\n",
      "Df Residuals:                   20631   BIC:                         4.534e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -36.9419      0.659    -56.067      0.000     -38.233     -35.650\n",
      "MedInc         0.4367      0.004    104.054      0.000       0.428       0.445\n",
      "HouseAge       0.0094      0.000     21.143      0.000       0.009       0.010\n",
      "AveRooms      -0.1073      0.006    -18.235      0.000      -0.119      -0.096\n",
      "AveBedrms      0.6451      0.028     22.928      0.000       0.590       0.700\n",
      "Population -3.976e-06   4.75e-06     -0.837      0.402   -1.33e-05    5.33e-06\n",
      "AveOccup      -0.0038      0.000     -7.769      0.000      -0.005      -0.003\n",
      "Latitude      -0.4213      0.007    -58.541      0.000      -0.435      -0.407\n",
      "Longitude     -0.4345      0.008    -57.682      0.000      -0.449      -0.420\n",
      "==============================================================================\n",
      "Omnibus:                     4393.650   Durbin-Watson:                   0.885\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14087.596\n",
      "Skew:                           1.082   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.420   Cond. No.                     2.38e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.38e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# using ols , i wann to generate summary to compare the t_statistics of mannual calucation and unsing function calculation \n",
    "\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164.54155727204218"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.36693293e-01/0.002654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.94192020718445"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.94192020718445"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76131542e-05, 1.99165349e-07, 3.46376893e-05, 7.91575022e-04,\n",
       "       2.25548856e-11, 2.37573240e-07, 5.17948835e-05, 5.67444334e-05])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(sse * np.linalg.inv(np.dot(X_diff.T, X_diff)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "X = np.array([2, 4, 6, 8, 10, 3, 5, 7, 9, 11])\n",
    "Y = np.array([70, 85, 90, 95, 100, 75, 80, 92, 94, 105])\n",
    "\n",
    "# Calculate mean of X and Y\n",
    "mean_X = np.mean(X)\n",
    "mean_Y = np.mean(Y)\n",
    "\n",
    "# Calculate the regression coefficients\n",
    "beta_1 = np.sum((X - mean_X) * (Y - mean_Y)) / np.sum((X - mean_X) ** 2)\n",
    "beta_0 = mean_Y - beta_1 * mean_X\n",
    "\n",
    "# Calculate predicted Y values\n",
    "Y_pred = beta_0 + beta_1 * X\n",
    "\n",
    "# Calculate explained sum of squares (ESS)\n",
    "ESS = np.sum((Y_pred - mean_Y) ** 2)\n",
    "\n",
    "# Calculate residual sum of squares (RSS)\n",
    "RSS = np.sum((Y - Y_pred) ** 2)\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df1 = 1  # Number of predictors\n",
    "df2 = len(X) - 1 - df1  # Total number of observations - Number of predictors - 1\n",
    "\n",
    "# Calculate mean squares\n",
    "MS1 = ESS / df1\n",
    "MS2 = RSS / df2\n",
    "\n",
    "# Calculate F-statistic\n",
    "F = MS1 / MS2\n",
    "\n",
    "# Output the results\n",
    "print(\"Regression Coefficients:\")\n",
    "print(\"beta_0:\", beta_0)\n",
    "print(\"beta_1:\", beta_1)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Sum of Squares:\")\n",
    "print(\"ESS:\", ESS)\n",
    "print(\"RSS:\", RSS)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Degrees of Freedom:\")\n",
    "print(\"df1:\", df1)\n",
    "print(\"df2:\", df2)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Mean Squares:\")\n",
    "print(\"MS1:\", MS1)\n",
    "print(\"MS2:\", MS2)\n",
    "print(\"\")\n",
    "\n",
    "print(\"F-statistic:\", F)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Input data\n",
    "method_A_scores = np.array([70, 85, 75])\n",
    "method_B_scores = np.array([90, 95, 80])\n",
    "method_C_scores = np.array([100, 92, 94, 105])\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(np.concatenate((method_A_scores, method_B_scores, method_C_scores)))\n",
    "\n",
    "# Calculate the group means\n",
    "group_A_mean = np.mean(method_A_scores)\n",
    "group_B_mean = np.mean(method_B_scores)\n",
    "group_C_mean = np.mean(method_C_scores)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = np.sum((np.concatenate((method_A_scores, method_B_scores, method_C_scores)) - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the between-group sum of squares (SSB)\n",
    "SSB = np.sum((np.array([group_A_mean] * len(method_A_scores)) - overall_mean) ** 2) + \\\n",
    "      np.sum((np.array([group_B_mean] * len(method_B_scores)) - overall_mean) ** 2) + \\\n",
    "      np.sum((np.array([group_C_mean] * len(method_C_scores)) - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the within-group sum of squares (SSW)\n",
    "SSW = np.sum((method_A_scores - group_A_mean) ** 2) + \\\n",
    "      np.sum((method_B_scores - group_B_mean) ** 2) + \\\n",
    "      np.sum((method_C_scores - group_C_mean) ** 2)\n",
    "\n",
    "#Calculate degrees of freedom\n",
    "df1 = 2 # Number of groups - 1\n",
    "df2 = len(method_A_scores) + len(method_B_scores) + len(method_C_scores) - df1 - 1\n",
    "\n",
    "#Calculate mean squares\n",
    "MSB = SSB / df1\n",
    "MSW = SSW / df2\n",
    "\n",
    "#Calculate F-statistic\n",
    "F = MSB / MSW\n",
    "\n",
    "#Calculate p-value\n",
    "p_value = 1 - f.cdf(F, df1, df2)\n",
    "\n",
    "#Output the results\n",
    "print(\"Between-Group Variation:\")\n",
    "print(\"SSB:\", SSB)\n",
    "print(\"df1:\", df1)\n",
    "print(\"MSB:\", MSB)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Within-Group Variation:\")\n",
    "print(\"SSW:\", SSW)\n",
    "print(\"df2:\", df2)\n",
    "print(\"MSW:\", MSW)\n",
    "print(\"\")\n",
    "\n",
    "print(\"F-statistic:\", F)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "F-statistic: 111.02064896755141\n",
      "\n",
      "ANOVA:\n",
      "Between-Group Variation:\n",
      "SSB: 452.3961111111109\n",
      "df1: 2\n",
      "MSB: 226.19805555555544\n",
      "\n",
      "Within-Group Variation:\n",
      "SSW: 338.08333333333337\n",
      "df2: 7\n",
      "MSW: 48.29761904761905\n",
      "F-statistic: 4.683420425601838\n",
      "p-value: 0.05116419342251366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Input data\n",
    "X = np.array([2, 4, 6, 8, 10, 3, 5, 7, 9, 11])\n",
    "Y = np.array([70, 85, 90, 95, 100, 75, 80, 92, 94, 105])\n",
    "method = np.array(['A', 'A', 'B', 'B', 'C', 'A', 'B', 'C', 'C', 'C'])\n",
    "\n",
    "# Linear Regression\n",
    "# Calculate the regression coefficients\n",
    "mean_X = np.mean(X)\n",
    "mean_Y = np.mean(Y)\n",
    "beta_1 = np.sum((X - mean_X) * (Y - mean_Y)) / np.sum((X - mean_X) ** 2)\n",
    "beta_0 = mean_Y - beta_1 * mean_X\n",
    "\n",
    "# Calculate predicted Y values\n",
    "Y_pred = beta_0 + beta_1 * X\n",
    "\n",
    "# Calculate the explained sum of squares (ESS) and residual sum of squares (RSS)\n",
    "ESS = np.sum((Y_pred - np.mean(Y)) ** 2)\n",
    "RSS = np.sum((Y - Y_pred) ** 2)\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df1_lr = 1  # Number of predictors (slope)\n",
    "df2_lr = len(X) - 1 - df1_lr  # Total number of observations - Number of predictors - 1\n",
    "\n",
    "# Calculate mean squares\n",
    "MS1_lr = ESS / df1_lr\n",
    "MS2_lr = RSS / df2_lr\n",
    "\n",
    "# Calculate F-statistic\n",
    "F_lr = MS1_lr / MS2_lr\n",
    "\n",
    "# ANOVA\n",
    "# Convert method to integers\n",
    "method_int = np.unique(method, return_inverse=True)[1]\n",
    "\n",
    "# Calculate group means\n",
    "group_means = []\n",
    "for group in np.unique(method_int):\n",
    "    group_means.append(np.mean(Y[method_int == group]))\n",
    "group_means = np.array(group_means)\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(Y)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = np.sum((Y - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the between-group sum of squares (SSB)\n",
    "SSB = np.sum((group_means - overall_mean) ** 2) * (len(np.unique(method_int)) - 1)\n",
    "\n",
    "# Calculate the within-group sum of squares (SSW)\n",
    "SSW = np.sum((Y - group_means[method_int]) ** 2)\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df1_anova = len(np.unique(method_int)) - 1  # Number of groups - 1\n",
    "df2_anova = len(X) - len(np.unique(method_int))  # Total number of observations - Number of groups\n",
    "\n",
    "# Calculate mean squares\n",
    "MSB = SSB / df1_anova\n",
    "MSW = SSW / df2_anova\n",
    "\n",
    "# Calculate F-statistic\n",
    "F_anova = MSB / MSW\n",
    "\n",
    "# Calculate p-value\n",
    "p_value_anova = 1 - f.cdf(F_anova, df1_anova, df2_anova)\n",
    "\n",
    "# Output the results\n",
    "print(\"Linear Regression:\")\n",
    "print(\"F-statistic:\", F_lr)\n",
    "print(\"\")\n",
    "\n",
    "print(\"ANOVA:\")\n",
    "print(\"Between-Group Variation:\")\n",
    "print(\"SSB:\", SSB)\n",
    "print(\"df1:\", df1_anova)\n",
    "print(\"MSB:\", MSB)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Within-Group Variation:\")\n",
    "print(\"SSW:\", SSW)\n",
    "print(\"df2:\", df2_anova)\n",
    "print(\"MSW:\", MSW)\n",
    "print(\"F-statistic:\", F_anova)\n",
    "print(\"p-value:\", p_value_anova)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "F-statistic: 111.02064896755141\n",
      "\n",
      "ANOVA:\n",
      "Between-Group Variation:\n",
      "SSB: 452.3961111111109\n",
      "df1: 2\n",
      "MSB: 226.19805555555544\n",
      "\n",
      "Within-Group Variation:\n",
      "SSW: 338.08333333333337\n",
      "df2: 7\n",
      "df2: 7\n",
      "MSW: 48.29761904761905\n",
      "F-statistic: 4.683420425601838\n",
      "p-value: 0.05116419342251366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Input data\n",
    "X = np.array([2, 4, 6, 8, 10, 3, 5, 7, 9, 11])\n",
    "Y = np.array([70, 85, 90, 95, 100, 75, 80, 92, 94, 105])\n",
    "method = np.array(['A', 'A', 'B', 'B', 'C', 'A', 'B', 'C', 'C', 'C'])\n",
    "\n",
    "# Linear Regression\n",
    "# Calculate the regression coefficients\n",
    "mean_X = np.mean(X)\n",
    "mean_Y = np.mean(Y)\n",
    "beta_1 = np.sum((X - mean_X) * (Y - mean_Y)) / np.sum((X - mean_X) ** 2)\n",
    "beta_0 = mean_Y - beta_1 * mean_X\n",
    "\n",
    "# Calculate predicted Y values\n",
    "Y_pred = beta_0 + beta_1 * X\n",
    "\n",
    "# Calculate the explained sum of squares (ESS) and residual sum of squares (RSS)\n",
    "ESS = np.sum((Y_pred - np.mean(Y)) ** 2)\n",
    "RSS = np.sum((Y - Y_pred) ** 2)\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df1_lr = 1  # Number of predictors (slope)\n",
    "df2_lr = len(X) - 1 - df1_lr  # Total number of observations - Number of predictors - 1\n",
    "\n",
    "# Calculate mean squares\n",
    "MS1_lr = ESS / df1_lr\n",
    "MS2_lr = RSS / df2_lr\n",
    "\n",
    "# Calculate F-statistic\n",
    "F_lr = MS1_lr / MS2_lr\n",
    "\n",
    "# ANOVA\n",
    "# Convert method to integers\n",
    "method_int = np.unique(method, return_inverse=True)[1]\n",
    "\n",
    "# Calculate group means\n",
    "group_means = []\n",
    "for group in np.unique(method_int):\n",
    "    group_means.append(np.mean(Y[method_int == group]))\n",
    "group_means = np.array(group_means)\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(Y)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = np.sum((Y - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the between-group sum of squares (SSB)\n",
    "SSB = np.sum((group_means - overall_mean) ** 2) * (len(np.unique(method_int)) - 1)\n",
    "\n",
    "# Calculate the within-group sum of squares (SSW)\n",
    "SSW = np.sum((Y - group_means[method_int]) ** 2)\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df1_anova = len(np.unique(method_int)) - 1  # Number of groups - 1\n",
    "df2_anova = len(Y) - len(np.unique(method_int))  # Total number of observations - Number of groups\n",
    "\n",
    "# Calculate mean squares\n",
    "MSB = SSB / df1_anova\n",
    "MSW = SSW / df2_anova\n",
    "\n",
    "# Calculate F-statistic\n",
    "F_anova = MSB / MSW\n",
    "\n",
    "# Calculate p-value\n",
    "p_value_anova = 1 - f.cdf(F_anova, df1_anova, df2_anova)\n",
    "\n",
    "# Output the results\n",
    "print(\"Linear Regression:\")\n",
    "print(\"F-statistic:\", F_lr)\n",
    "print(\"\")\n",
    "\n",
    "print(\"ANOVA:\")\n",
    "print(\"Between-Group Variation:\")\n",
    "print(\"SSB:\", SSB)\n",
    "print(\"df1:\", df1_anova)\n",
    "print(\"MSB:\", MSB)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Within-Group Variation:\")\n",
    "print(\"SSW:\", SSW)\n",
    "print(\"df2:\", df2_anova)\n",
    "print(\"df2:\", df2_anova)\n",
    "print(\"MSW:\", MSW)\n",
    "\n",
    "print(\"F-statistic:\", F_anova)\n",
    "print(\"p-value:\", p_value_anova)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
